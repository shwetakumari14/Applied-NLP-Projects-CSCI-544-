{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F-q4c4BuEvOB"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ITNErb6fE7rM"
   },
   "outputs": [],
   "source": [
    "noun_list = [\"tion\", \"ity\", \"er\", \"ness\", \"ism\", \"ment\", \"ant\", \"ship\", \"age\", \"ery\"]\n",
    "verb_list = [\"ate\", \"ify\", \"ize\", \"ise\"]\n",
    "adj_list = [\"able\", \"ible\", 'ant', 'ent', 'ive', \"al\",\"ial\",\"an\",\"ian\",\"ish\", \"ern\", \"ese\", \"ful\", 'ar', 'ary','ly','less','ic','ive','ous', \"i\", \"ic\"]\n",
    "adv_list = [\"ly\",\"lng\",\"ward\", \"wards\", \"way\", \"ways\", \"wise\"]\n",
    "\n",
    "def processUnknowns(word):\n",
    "    num = 0\n",
    "    for char in word:\n",
    "      if char.isdigit():\n",
    "         num += 1\n",
    "          \n",
    "    fraction = num / float(len(word))\n",
    "        \n",
    "    if word.isdigit():\n",
    "        return \"<unk_num>\"\n",
    "    elif fraction > 0.5:\n",
    "        return \"<unk_mainly_num>\"\n",
    "    elif any(word.endswith(suffix) for suffix in verb_list):\n",
    "        return \"<unk_verb>\"\n",
    "    elif any(word.endswith(suffix) for suffix in adj_list):\n",
    "        return \"<unk_adj>\"\n",
    "    elif any(word.endswith(suffix) for suffix in adv_list):\n",
    "        return \"<unk_adv>\"\n",
    "    elif word.islower():\n",
    "        return \"<unk_all_lower>\"    \n",
    "    elif word.isupper():\n",
    "        return \"<unk_all_upper>\"              \n",
    "    elif word[0].isupper():\n",
    "        return \"<unk_initial_upper>\"\n",
    "    elif any(char.isdigit() for char in word):\n",
    "        return \"<unk_contain_num>\"    \n",
    "    else:\n",
    "        return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XRb4dRRTFaPv"
   },
   "outputs": [],
   "source": [
    "def prepareVocabulary(file, min_count=2):\n",
    "    vocab, NER_set, sentence, sentences = {}, set(), [], []\n",
    "    with open(file, \"r\") as train:\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                sentences.append(sentence)\n",
    "                sentence =[]\n",
    "                continue\n",
    "            word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "            if word_type not in vocab:\n",
    "                vocab[word_type] = 1\n",
    "            else:\n",
    "                vocab[word_type]+=1\n",
    "            sentence.append([word_type,NER_type])\n",
    "            NER_set.add(NER_type)\n",
    "        sentences.append(sentence)\n",
    "                \n",
    "        vocab['<unk>'], vocab['<unk_mainly_num>'] = 0,0\n",
    "        vocab['<unk_num>'], vocab['<unk_contain_num>'] = 0,0\n",
    "        vocab['<unk_verb>'], vocab['<unk_adj>'] = 0,0\n",
    "        vocab['<unk_adv>'], vocab['<unk_all_lower>'] = 0,0\n",
    "        vocab['<unk_all_upper>'], vocab['<unk_initial_upper>'] = 0,0\n",
    "        \n",
    "        delete = []\n",
    "        for word, occurrences in vocab.items():\n",
    "            if occurrences >= min_count: \n",
    "                continue\n",
    "            else:\n",
    "                new_token = processUnknowns(word)\n",
    "                vocab[new_token] += occurrences \n",
    "                delete.append(word)\n",
    "\n",
    "        for i in delete:  \n",
    "            del vocab[i]\n",
    "    \n",
    "    return vocab, NER_set, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pp5OGQtVFtcG",
    "outputId": "e58ab3e7-27e8-4a6a-b125-d3e4080cb95f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994\n"
     ]
    }
   ],
   "source": [
    "vocab, NER_set, sentences = prepareVocabulary('/content/drive/MyDrive/Colab Notebooks/data/train')\n",
    "sortedVocabulary = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "Word_to_Index = {w: i+1 for i, (w, n) in enumerate(sortedVocabulary)}\n",
    "Word_to_Index['PAD'] = 0\n",
    "print(len(Word_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6-ZVKFoFwDT",
    "outputId": "c7511c52-8282-4f0a-9ed6-53f04e37c840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "NER_to_Index = {}\n",
    "i = 0\n",
    "for ner in NER_set:\n",
    "    NER_to_Index[ner] = i\n",
    "    i += 1\n",
    "\n",
    "Index_to_Word = {}\n",
    "for key, value in Word_to_Index.items():\n",
    "    Index_to_Word[value] = key\n",
    "\n",
    "Index_to_Ner = {}\n",
    "for key, value in NER_to_Index.items():\n",
    "    Index_to_Ner[value] = key\n",
    "\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VfZBwRFSF0JW"
   },
   "outputs": [],
   "source": [
    "data_X = []\n",
    "\n",
    "for s in sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in Word_to_Index:\n",
    "            temp_X.append(Word_to_Index.get(w))\n",
    "        else:\n",
    "            unk = processUnknowns(w)\n",
    "            temp_X.append(Word_to_Index[unk])\n",
    "    data_X.append(temp_X)\n",
    "\n",
    "data_y = []\n",
    "for s in sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(NER_to_Index.get(label))\n",
    "    data_y.append(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXeR3AG-F3Ti",
    "outputId": "833bbe34-f673-4f01-ee59-2dca9ab38810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "def padding_for_words(dataset, max_len):\n",
    "    for i, line in enumerate(dataset):\n",
    "        if len(line) > max_len:\n",
    "            dataset[i] = line[:max_len]\n",
    "        elif len(line) < max_len: \n",
    "            dataset[i] = line[:len(line)] + [0]*(max_len-len(line))\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding_for_NER(dataset, max_len):\n",
    "    for i, line in enumerate(dataset):\n",
    "        if len(line) > max_len:\n",
    "            dataset[i] = line[:max_len]\n",
    "        elif len(line) < max_len:\n",
    "            dataset[i] = line[:len(line)] + [-100]*(max_len-len(line))\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "data_X = padding_for_words(data_X, 130) \n",
    "data_y = padding_for_NER(data_y, 130)\n",
    "X_train = torch.LongTensor(data_X)\n",
    "Y_train = torch.LongTensor(data_y)\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=10, shuffle=False)\n",
    "\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59LWWOlIF7S_",
    "outputId": "babc1000-061c-481e-f434-f1c6f6e28b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cuda --\n"
     ]
    }
   ],
   "source": [
    "isCuda = torch.cuda.is_available()\n",
    "\n",
    "if isCuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"-- cuda --\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"-- cpu --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "e0iO1rJZF-pP"
   },
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, first_output_dim, output_dim, num_layers, bidirectional, drop_out): \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, first_output_dim)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = nn.ELU()\n",
    "        self.fc2 = nn.Linear(first_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.blstm(embedded)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.activation(self.fc1(outputs))\n",
    "        predictions = self.fc2(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SL-hVIDjGA9Y",
    "outputId": "5f8e4b3a-3ddc-40ef-fabc-eed588e24c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(Word_to_Index)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "FIRST_OUTPUT_DIM = 128\n",
    "OUTPUT_DIM = len(NER_to_Index)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.33\n",
    "\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "\n",
    "model.to(device)\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_kzGRPdV8iBZ"
   },
   "outputs": [],
   "source": [
    "def categoricalAccuracy(preds, y, tag_pad_idx, text, predict_table):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) \n",
    "    for predict, real, word in zip(max_preds, y, text):\n",
    "        if real.item() == tag_pad_idx:\n",
    "            continue\n",
    "        else:\n",
    "            predict_table.append((word.item(), predict.item(), real.item()))\n",
    "            if real.item() == predict.item():\n",
    "                correct += 1\n",
    "            tot += 1\n",
    "    return tot, correct, predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aiOKzWsRGDZK"
   },
   "outputs": [],
   "source": [
    "def trainModel(model, dataloader, predict_table):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.train()\n",
    "\n",
    "    for text, tags in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        tags = tags.to(device)\n",
    "        text = text.to(device)   \n",
    "        predictions = model(text)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1]) \n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, tags)\n",
    "\n",
    "        tot, correct, predict_table = categoricalAccuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += correct\n",
    "        epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / epoch_tot, predict_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "53A0g7rcGGZ6"
   },
   "outputs": [],
   "source": [
    "def evaluateModel(model, dataloader, predict_table):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text, tags in dataloader:\n",
    "            tags = tags.to(device)\n",
    "            text = text.to(device)\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            tot, correct, predict_table = categoricalAccuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "            epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / epoch_tot, predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "i5nHGdtmGMWn"
   },
   "outputs": [],
   "source": [
    "dev_sentences = []\n",
    "sentence=[]\n",
    "cnt=0\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as dev:\n",
    "    for line in dev:\n",
    "        if not line.split():\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence =[]\n",
    "            continue\n",
    "        word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        cnt+=1\n",
    "        sentence.append([word_type,NER_type])\n",
    "    dev_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HqpSm5KXGeLB"
   },
   "outputs": [],
   "source": [
    "dev_X = []\n",
    "for s in dev_sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in Word_to_Index:\n",
    "            temp_X.append(Word_to_Index.get(w))\n",
    "        else:\n",
    "            unk = processUnknowns(w)\n",
    "            temp_X.append(Word_to_Index[unk])\n",
    "    dev_X.append(temp_X)\n",
    "\n",
    "dev_y = []\n",
    "for s in dev_sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(NER_to_Index.get(label))\n",
    "    dev_y.append(temp_y)\n",
    "\n",
    "dev_X = padding_for_words(dev_X, 130)\n",
    "dev_y = padding_for_NER(dev_y, 130)\n",
    "\n",
    "X_dev = torch.LongTensor(dev_X)\n",
    "Y_dev = torch.LongTensor(dev_y)\n",
    "ds_dev = TensorDataset(X_dev, Y_dev)\n",
    "loader_dev = DataLoader(ds_dev, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bHyQidQGqHU",
    "outputId": "508de374-62ac-445d-863e-7fb7e523f054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.644 | Train Acc: 85.03%\n",
      "\t Val. Loss: 0.448 |  Val. Acc: 88.28%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.442 | Train Acc: 87.91%\n",
      "\t Val. Loss: 0.307 |  Val. Acc: 91.27%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.348 | Train Acc: 89.77%\n",
      "\t Val. Loss: 0.244 |  Val. Acc: 93.03%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.296 | Train Acc: 90.91%\n",
      "\t Val. Loss: 0.210 |  Val. Acc: 93.89%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.261 | Train Acc: 91.80%\n",
      "\t Val. Loss: 0.198 |  Val. Acc: 94.19%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.234 | Train Acc: 92.50%\n",
      "\t Val. Loss: 0.179 |  Val. Acc: 94.66%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.217 | Train Acc: 92.94%\n",
      "\t Val. Loss: 0.165 |  Val. Acc: 95.01%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.201 | Train Acc: 93.36%\n",
      "\t Val. Loss: 0.159 |  Val. Acc: 95.22%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.190 | Train Acc: 93.70%\n",
      "\t Val. Loss: 0.151 |  Val. Acc: 95.47%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.180 | Train Acc: 93.90%\n",
      "\t Val. Loss: 0.147 |  Val. Acc: 95.61%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.171 | Train Acc: 94.19%\n",
      "\t Val. Loss: 0.143 |  Val. Acc: 95.69%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.165 | Train Acc: 94.39%\n",
      "\t Val. Loss: 0.138 |  Val. Acc: 95.94%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.157 | Train Acc: 94.52%\n",
      "\t Val. Loss: 0.144 |  Val. Acc: 95.74%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.151 | Train Acc: 94.79%\n",
      "\t Val. Loss: 0.139 |  Val. Acc: 95.87%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.145 | Train Acc: 94.94%\n",
      "\t Val. Loss: 0.133 |  Val. Acc: 96.03%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.140 | Train Acc: 95.09%\n",
      "\t Val. Loss: 0.134 |  Val. Acc: 96.14%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.135 | Train Acc: 95.18%\n",
      "\t Val. Loss: 0.136 |  Val. Acc: 96.09%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.131 | Train Acc: 95.35%\n",
      "\t Val. Loss: 0.132 |  Val. Acc: 96.19%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.127 | Train Acc: 95.49%\n",
      "\t Val. Loss: 0.132 |  Val. Acc: 96.25%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.123 | Train Acc: 95.58%\n",
      "\t Val. Loss: 0.133 |  Val. Acc: 96.12%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "tag_pad_idx=-100\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= -100)\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_predict_table = []\n",
    "    test_predict_table = []\n",
    "\n",
    "    train_loss, train_acc, train_predict_table = trainModel(model, loader_train, train_predict_table)\n",
    "    valid_loss, valid_acc, valid_predict_table = evaluateModel(model, loader_dev, test_predict_table)\n",
    "\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_predict_table = valid_predict_table\n",
    "        torch.save(model.state_dict(), './blstm1.pt')\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AKF2ysJvH5Gu"
   },
   "outputs": [],
   "source": [
    "term = [int(x[0]) for x in best_predict_table]\n",
    "y_pred = [int(x[1]) for x in best_predict_table]\n",
    "i=0\n",
    "newfile = open('./dev1.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(Index_to_Ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()\n",
    "\n",
    "i=0\n",
    "newfile = open('./dev1_perl.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type, NER_type = line.split(\" \")[0], line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(NER_type)+' '+str(Index_to_Ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvEz_6GDYWvi",
    "outputId": "0f2f46fb-052a-401c-a24f-5238431dcf1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5609 phrases; correct: 4567.\n",
      "accuracy:  96.19%; precision:  81.42%; recall:  76.86%; FB1:  79.08\n",
      "              LOC: precision:  87.78%; recall:  84.10%; FB1:  85.90  1760\n",
      "             MISC: precision:  73.21%; recall:  76.46%; FB1:  74.80  963\n",
      "              ORG: precision:  72.74%; recall:  64.28%; FB1:  68.25  1185\n",
      "              PER: precision:  85.54%; recall:  78.99%; FB1:  82.13  1701\n"
     ]
    }
   ],
   "source": [
    "!perl conll03eval.txt < dev1_perl.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xSqbxuwhQIaG"
   },
   "outputs": [],
   "source": [
    "def categoricalEvaluate(preds, text, predictTable):\n",
    "\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    for predict, word in zip(max_preds, text):\n",
    "        if word == 0:\n",
    "            continue\n",
    "        else:\n",
    "            predictTable.append((word, predict[0]))\n",
    "\n",
    "    return predictTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MrhS9hFIQ4C-"
   },
   "outputs": [],
   "source": [
    "def evaluateModel(model, loader, predictTable):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text in loader:\n",
    "            text = text.to(device)\n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "\n",
    "            predictTable = categoricalEvaluate(predictions, text.view(-1), predictTable)\n",
    "\n",
    "    return predictTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LBwH36R6Q7Vh"
   },
   "outputs": [],
   "source": [
    "test_X = []\n",
    "sentence = []\n",
    "cnt=0\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            test_X.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        word_type = line.split(\" \")[1]\n",
    "        if word_type in Word_to_Index:\n",
    "            sentence.append(Word_to_Index.get(word_type))\n",
    "        else:\n",
    "            unk = processUnknowns(word_type)\n",
    "            sentence.append(Word_to_Index.get(unk))\n",
    "    test_X.append(sentence)\n",
    "\n",
    "test_X = padding_for_words(test_X, 130)\n",
    "X_test = torch.LongTensor(test_X)\n",
    "loader_test = DataLoader(X_test, batch_size=10, shuffle=False)\n",
    "\n",
    "evaluate_predict_table2 = []\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./blstm1.pt'))\n",
    "prediction_table = evaluateModel(model, loader_test, evaluate_predict_table2)\n",
    "\n",
    "term = [int(x[0]) for x in evaluate_predict_table2]\n",
    "y_pred = [int(x[1]) for x in evaluate_predict_table2]\n",
    "\n",
    "i=0\n",
    "newfile = open('./test1.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        for_tag = Index_to_Ner[y_pred[i]]\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+for_tag+'\\n')\n",
    "        i += 1\n",
    "newfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lI4ST5tSNm4f"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./vocab_dictionary.pickle','wb') as fw1:\n",
    "    pickle.dump(Word_to_Index, fw1)\n",
    "with open('./ner_dictionary.pickle','wb') as fw2:\n",
    "    pickle.dump(NER_to_Index, fw2)\n",
    "with open('./int_vocab_dictionary.pickle','wb') as fw3:\n",
    "    pickle.dump(Index_to_Word, fw3)\n",
    "with open('./int_ner_dictionary.pickle','wb') as fw4:\n",
    "    pickle.dump(Index_to_Ner, fw4)\n",
    "with open('./loader_train.pickle','wb') as fw5:\n",
    "    pickle.dump(loader_train, fw5)\n",
    "with open('./loader_dev.pickle','wb') as fw6:\n",
    "    pickle.dump(loader_dev, fw6)\n",
    "with open('./loader_test.pickle','wb') as fw7:\n",
    "    pickle.dump(loader_test, fw7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HqEeOyUEi_AK"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'INPUT_DIM':len(Word_to_Index),\n",
    "              'EMBEDDING_DIM':100,\n",
    "              'HIDDEN_DIM':256,\n",
    "              'FIRST_OUTPUT_DIM':128,\n",
    "              'OUTPUT_DIM':len(NER_to_Index),\n",
    "              'N_LAYERS':1,\n",
    "              'BIDIRECTIONAL':True,\n",
    "              'DROPOUT':0.33,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, './checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HrMvCXZjNL7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
