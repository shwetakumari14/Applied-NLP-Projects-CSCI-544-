{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SOfZRm1ZSiv9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = [\"tion\", \"ity\", \"er\", \"ness\", \"ism\", \"ment\", \"ant\", \"ship\", \"age\", \"ery\"]\n",
    "verb_list = [\"ate\", \"ify\", \"ize\", \"ise\"]\n",
    "adj_list = [\"able\", \"ible\", 'ant', 'ent', 'ive', \"al\",\"ial\",\"an\",\"ian\",\"ish\", \"ern\", \"ese\", \"ful\", 'ar', 'ary','ly','less','ic','ive','ous', \"i\", \"ic\"]\n",
    "adv_list = [\"ly\",\"lng\",\"ward\", \"wards\", \"way\", \"ways\", \"wise\"]\n",
    "\n",
    "def processUnknowns(word):\n",
    "    num = 0\n",
    "    for char in word:\n",
    "      if char.isdigit():\n",
    "         num += 1\n",
    "          \n",
    "    fraction = num / float(len(word))\n",
    "        \n",
    "    if word.isdigit():\n",
    "        return \"<unk_num>\"\n",
    "    elif fraction > 0.5:\n",
    "        return \"<unk_mainly_num>\"\n",
    "    elif any(word.endswith(suffix) for suffix in verb_list):\n",
    "        return \"<unk_verb>\"\n",
    "    elif any(word.endswith(suffix) for suffix in adj_list):\n",
    "        return \"<unk_adj>\"\n",
    "    elif any(word.endswith(suffix) for suffix in adv_list):\n",
    "        return \"<unk_adv>\"\n",
    "    elif word.islower():\n",
    "        return \"<unk_all_lower>\"    \n",
    "    elif word.isupper():\n",
    "        return \"<unk_all_upper>\"              \n",
    "    elif word[0].isupper():\n",
    "        return \"<unk_initial_upper>\"\n",
    "    elif any(char.isdigit() for char in word):\n",
    "        return \"<unk_contain_num>\"    \n",
    "    else:\n",
    "        return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ClJPjJ1IZWr"
   },
   "outputs": [],
   "source": [
    "def prepareVocabulary(file, min_count=2):\n",
    "    vocab, NER_set, sentence, sentences = {}, set(), [], []\n",
    "    with open(file, \"r\") as train:\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                sentences.append(sentence)\n",
    "                sentence =[]\n",
    "                continue\n",
    "            word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "            if word_type not in vocab:\n",
    "                vocab[word_type] = 1\n",
    "            else:\n",
    "                vocab[word_type]+=1\n",
    "            sentence.append([word_type,NER_type])\n",
    "            NER_set.add(NER_type)\n",
    "        sentences.append(sentence)\n",
    "                \n",
    "        vocab['<unk>'], vocab['<unk_mainly_num>'] = 0,0\n",
    "        vocab['<unk_num>'], vocab['<unk_contain_num>'] = 0,0\n",
    "        vocab['<unk_verb>'], vocab['<unk_adj>'] = 0,0\n",
    "        vocab['<unk_adv>'], vocab['<unk_all_lower>'] = 0,0\n",
    "        vocab['<unk_all_upper>'], vocab['<unk_initial_upper>'] = 0,0\n",
    "        \n",
    "        delete = []\n",
    "        for word, occurrences in vocab.items():\n",
    "            if occurrences >= min_count: \n",
    "                continue\n",
    "            else:\n",
    "                new_token = processUnknowns(word)\n",
    "                vocab[new_token] += occurrences \n",
    "                delete.append(word)\n",
    "\n",
    "        for i in delete:  \n",
    "            del vocab[i]\n",
    "    \n",
    "    return vocab, NER_set, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsjLeLOMIiZ7",
    "outputId": "bc5d408e-de0f-48fa-acc5-de1706df2d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994\n"
     ]
    }
   ],
   "source": [
    "vocab, NER_set, sentences = prepareVocabulary('/content/drive/MyDrive/Colab Notebooks/data/train')\n",
    "sortedVocabulary = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "Word_to_Index = {w: i+1 for i, (w, n) in enumerate(sortedVocabulary)}\n",
    "Word_to_Index['PAD'] = 0\n",
    "print(len(Word_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDCeZX4tIykV",
    "outputId": "2f524a26-ef87-4311-c359-f4ab91354423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "NER_to_Index = {}\n",
    "i = 0\n",
    "for ner in NER_set:\n",
    "    NER_to_Index[ner] = i\n",
    "    i += 1\n",
    "\n",
    "Index_to_Word = {}\n",
    "for key, value in Word_to_Index.items():\n",
    "    Index_to_Word[value] = key\n",
    "\n",
    "Index_to_Ner = {}\n",
    "for key, value in NER_to_Index.items():\n",
    "    Index_to_Ner[value] = key\n",
    "\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CLW6PE-qI4BS"
   },
   "outputs": [],
   "source": [
    "data_X = []\n",
    "\n",
    "for s in sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in Word_to_Index:\n",
    "            temp_X.append(Word_to_Index.get(w))\n",
    "        else:\n",
    "            unk = processUnknowns(w)\n",
    "            temp_X.append(Word_to_Index[unk])\n",
    "    data_X.append(temp_X)\n",
    "\n",
    "data_y = []\n",
    "for s in sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(NER_to_Index.get(label))\n",
    "    data_y.append(temp_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgC4oACuI650",
    "outputId": "6cc146c4-6e48-43c9-bfe6-f14f039551cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "def padding_for_words(dataset, max_len):\n",
    "    for i, line in enumerate(dataset):\n",
    "        if len(line) > max_len:\n",
    "            dataset[i] = line[:max_len]\n",
    "        elif len(line) < max_len: \n",
    "            dataset[i] = line[:len(line)] + [0]*(max_len-len(line))\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding_for_NER(dataset, max_len):\n",
    "    for i, line in enumerate(dataset):\n",
    "        if len(line) > max_len:\n",
    "            dataset[i] = line[:max_len]\n",
    "        elif len(line) < max_len:\n",
    "            dataset[i] = line[:len(line)] + [-100]*(max_len-len(line))\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "data_X = padding_for_words(data_X, 130) \n",
    "data_y = padding_for_NER(data_y, 130)\n",
    "X_train = torch.LongTensor(data_X)\n",
    "Y_train = torch.LongTensor(data_y)\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=16, shuffle=False)\n",
    "\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dbIYsIc2I83q"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "with gzip.open('/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.gz', 'rb') as f_in:\n",
    "    with open('glove.6B.100d', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "embedding_dict = dict()\n",
    "f = open(os.path.join('glove.6B.100d'), encoding='utf-8')\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') \n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(Word_to_Index), embedding_dim))\n",
    "\n",
    "for word, i in Word_to_Index.items():\n",
    "    embedding_vector = embedding_dict.get(word.lower())\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix = torch.LongTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxBycDUAI-eA",
    "outputId": "6b48de34-80e9-4691-e263-d7589f614b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- cuda ---\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"-- cuda ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"--- cpu ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6gH8wntcJLnU"
   },
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, first_output_dim, output_dim, num_layers, bidirectional, drop_out): \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, first_output_dim)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = nn.ELU()\n",
    "        self.fc2 = nn.Linear(first_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.blstm(embedded)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.activation(self.fc1(outputs))\n",
    "        predictions = self.fc2(outputs)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oa234j4JnPs",
    "outputId": "eb2854f9-a222-4659-92e6-05015915fa1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11994 9\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(Word_to_Index)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "FIRST_OUTPUT_DIM = 128\n",
    "OUTPUT_DIM = len(NER_to_Index)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.33\n",
    "\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "\n",
    "model.to(device)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "\n",
    "print(len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Fd0Y3fwCJpCO"
   },
   "outputs": [],
   "source": [
    "def trainModel(model, dataloader, predict_table):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.train()\n",
    "\n",
    "    for text, tags in dataloader:\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        tags = tags.to(device)\n",
    "        text = text.to(device)     \n",
    "        predictions = model(text)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(predictions, tags)\n",
    "        tot, correct, predict_table = categoricalAccuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += correct\n",
    "        epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / epoch_tot, predict_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Xpbii3n_JrqI"
   },
   "outputs": [],
   "source": [
    "def categoricalAccuracy(preds, y, tag_pad_idx, text, predict_table):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    for predict, real, word in zip(max_preds, y, text):\n",
    "        if real.item() == tag_pad_idx:\n",
    "            continue\n",
    "        else:\n",
    "            predict_table.append((word.item(), predict.item(), real.item()))\n",
    "            if real.item() == predict.item():\n",
    "                correct += 1\n",
    "            tot += 1\n",
    "    return tot, correct, predict_table\n",
    "\n",
    "def model_evaluate(model, dataloader, predict_table):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text, tags in dataloader:\n",
    "            tags = tags.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            tot, correct, predict_table = categoricalAccuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "            epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_acc / epoch_tot, predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "plK4KyJEJuIj"
   },
   "outputs": [],
   "source": [
    "dev_sentences = []\n",
    "sentence=[]\n",
    "cnt=0\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as dev:\n",
    "    for line in dev:\n",
    "        if not line.split():\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence =[]\n",
    "            continue\n",
    "        word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        cnt+=1\n",
    "        sentence.append([word_type,NER_type])\n",
    "    dev_sentences.append(sentence)\n",
    "\n",
    "dev_X = []\n",
    "for s in dev_sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in Word_to_Index:\n",
    "            temp_X.append(Word_to_Index.get(w))\n",
    "        else:\n",
    "            unk = processUnknowns(w)\n",
    "            temp_X.append(Word_to_Index[unk])\n",
    "    dev_X.append(temp_X)\n",
    "\n",
    "dev_y = []\n",
    "for s in dev_sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(NER_to_Index.get(label))\n",
    "    dev_y.append(temp_y)\n",
    "\n",
    "dev_X = padding_for_words(dev_X, 130)\n",
    "dev_y = padding_for_NER(dev_y, 130)\n",
    "\n",
    "X_dev = torch.LongTensor(dev_X)\n",
    "Y_dev = torch.LongTensor(dev_y)\n",
    "ds_dev = TensorDataset(X_dev, Y_dev)\n",
    "loader_dev = DataLoader(ds_dev, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYL4ikaoaWCC",
    "outputId": "a61135de-0ce0-42fd-c077-af4abbd0b340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(\"---- \", type(loader_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Gxycx7J0Y6",
    "outputId": "21d498d5-9333-4ce6-efe9-b29d8e1b775e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.436 | Train Acc: 88.63%\n",
      "\t Val. Loss: 0.238 |  Val. Acc: 93.47%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.201 | Train Acc: 93.84%\n",
      "\t Val. Loss: 0.149 |  Val. Acc: 95.70%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.129 | Train Acc: 95.85%\n",
      "\t Val. Loss: 0.122 |  Val. Acc: 96.27%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.099 | Train Acc: 96.76%\n",
      "\t Val. Loss: 0.113 |  Val. Acc: 96.45%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.37%\n",
      "\t Val. Loss: 0.112 |  Val. Acc: 96.36%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.067 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.103 |  Val. Acc: 96.61%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.059 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.100 |  Val. Acc: 96.72%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.32%\n",
      "\t Val. Loss: 0.097 |  Val. Acc: 96.97%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.47%\n",
      "\t Val. Loss: 0.101 |  Val. Acc: 96.92%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.60%\n",
      "\t Val. Loss: 0.097 |  Val. Acc: 97.00%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.74%\n",
      "\t Val. Loss: 0.103 |  Val. Acc: 96.96%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.82%\n",
      "\t Val. Loss: 0.098 |  Val. Acc: 97.05%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.88%\n",
      "\t Val. Loss: 0.100 |  Val. Acc: 97.15%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.06%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.44%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.10%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.46%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.14%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.50%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.14%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.50%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.15%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.52%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.18%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.52%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.47%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.21%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.53%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.56%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.23%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.58%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.26%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.58%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.25%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.23%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.58%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.022 | Train Acc: 99.21%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.58%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.27%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.60%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.25%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.25%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.59%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.021 | Train Acc: 99.23%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.60%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 35\n",
    "tag_pad_idx=-100\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.23, momentum=0.9, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= -100)\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_predict_table = []\n",
    "    test_predict_table = []\n",
    "\n",
    "    train_loss, train_acc, train_predict_table = trainModel(model, loader_train, train_predict_table)\n",
    "    valid_loss, valid_acc, valid_predict_table = model_evaluate(model, loader_dev, test_predict_table)\n",
    "\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_predict_table = valid_predict_table\n",
    "        torch.save(model.state_dict(), './blstm2.pt')\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0pQ2dX9kJ22v"
   },
   "outputs": [],
   "source": [
    "def categoricalEvaluate(preds, text, predict_table):\n",
    "\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    for predict, word in zip(max_preds, text):\n",
    "        if word == 0:\n",
    "            continue\n",
    "        else:\n",
    "            predict_table.append((word, predict[0]))\n",
    "\n",
    "    return predict_table\n",
    "\n",
    "def model_evaluate(model, dataloader, predict_table):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in dataloader:\n",
    "            text = text.to(device)\n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            predict_table = categoricalEvaluate(predictions, text.view(-1), predict_table)\n",
    "\n",
    "    return predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "abC-gAqcUMsE"
   },
   "outputs": [],
   "source": [
    "term = [int(x[0]) for x in best_predict_table]\n",
    "y_pred = [int(x[1]) for x in best_predict_table]\n",
    "i=0\n",
    "newfile = open('./dev2.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(Index_to_Ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()\n",
    "\n",
    "i=0\n",
    "newfile = open('./dev2_perl.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type, NER_type = line.split(\" \")[0], line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(NER_type)+' '+str(Index_to_Ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDakQBN_Usw4",
    "outputId": "bc423d70-3353-4cec-fb7b-cae7490582f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51578 tokens with 5942 phrases; found: 5687 phrases; correct: 5061.\n",
      "accuracy:  97.59%; precision:  88.99%; recall:  85.17%; FB1:  87.04\n",
      "              LOC: precision:  91.37%; recall:  91.67%; FB1:  91.52  1843\n",
      "             MISC: precision:  83.71%; recall:  81.34%; FB1:  82.51  896\n",
      "              ORG: precision:  83.40%; recall:  79.79%; FB1:  81.55  1283\n",
      "              PER: precision:  93.51%; recall:  84.53%; FB1:  88.79  1665\n"
     ]
    }
   ],
   "source": [
    "!perl conll03eval.txt < dev2_perl.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRAe6XspH7WI",
    "outputId": "b9fd5d2a-649e-4120-ac6b-9d4726c6b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****  11994 9\n"
     ]
    }
   ],
   "source": [
    "print(\"**** \", len(Word_to_Index), len(NER_to_Index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RUdibStHU_gb"
   },
   "outputs": [],
   "source": [
    "test_X = []\n",
    "sentence = []\n",
    "cnt=0\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/test', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            test_X.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        word_type = line.split(\" \")[1]\n",
    "        if word_type in Word_to_Index:\n",
    "            sentence.append(Word_to_Index.get(word_type))\n",
    "        else:\n",
    "            unk = processUnknowns(word_type)\n",
    "            sentence.append(Word_to_Index.get(unk))\n",
    "    test_X.append(sentence)\n",
    "\n",
    "test_X = padding_for_words(test_X, 130)\n",
    "X_test = torch.LongTensor(test_X)\n",
    "loader_test = DataLoader(X_test, batch_size=16, shuffle=False)\n",
    "\n",
    "evaluate_predict_table2 = []\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "model.to(device)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "model.load_state_dict(torch.load('./blstm2.pt'))\n",
    "prediction_table = model_evaluate(model, loader_test, evaluate_predict_table2)\n",
    "\n",
    "term = [int(x[0]) for x in evaluate_predict_table2]\n",
    "y_pred = [int(x[1]) for x in evaluate_predict_table2]\n",
    "\n",
    "i=0\n",
    "newfile = open('./test2.out', \"w\")\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/data/test', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        for_tag = Index_to_Ner[y_pred[i]]\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+for_tag+'\\n')\n",
    "        i += 1\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84UuJDADYASP",
    "outputId": "265acaa4-f413-49a4-8a8d-e040793a33c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "lWFlNck2VNzc"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save data\n",
    "with open('./vocab_dictionary1.pickle','wb') as fw1:\n",
    "    pickle.dump(Word_to_Index, fw1)\n",
    "with open('./ner_dictionary1.pickle','wb') as fw2:\n",
    "    pickle.dump(NER_to_Index, fw2)\n",
    "with open('./int_vocab_dictionary1.pickle','wb') as fw3:\n",
    "    pickle.dump(Index_to_Word, fw3)\n",
    "with open('./int_ner_dictionary1.pickle','wb') as fw4:\n",
    "    pickle.dump(Index_to_Ner, fw4)\n",
    "with open('./loader_train1.pickle','wb') as fw5:\n",
    "    pickle.dump(loader_train, fw5)\n",
    "with open('./loader_dev1.pickle','wb') as fw6:\n",
    "    pickle.dump(loader_dev, fw6)\n",
    "with open('./loader_test1.pickle','wb') as fw7:\n",
    "    pickle.dump(loader_test, fw7)\n",
    "with open('./embedding_matrix.pickle','wb') as fw8:\n",
    "    pickle.dump(embedding_matrix, fw8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "vjg1j-8UWcpq"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'INPUT_DIM':len(Word_to_Index),\n",
    "              'EMBEDDING_DIM':100,\n",
    "              'HIDDEN_DIM':256,\n",
    "              'FIRST_OUTPUT_DIM':128,\n",
    "              'OUTPUT_DIM':len(NER_to_Index),\n",
    "              'N_LAYERS':1,\n",
    "              'BIDIRECTIONAL':True,\n",
    "              'DROPOUT':0.33,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, './checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7iv3UjgRIXL1"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "8XMaTxdXWgJ0"
   },
   "outputs": [],
   "source": [
    "with open('./best_predict_table.pickle','wb') as fw9:\n",
    "    pickle.dump(best_predict_table, fw9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrUixUyssqzR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
