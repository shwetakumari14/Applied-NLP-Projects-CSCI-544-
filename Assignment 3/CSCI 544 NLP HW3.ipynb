{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c25454",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation\n",
    "\n",
    "- In order to prepare the data before preprocessing first I read columns \"review_body\", and \"star_rating\" from the CSV file to store as my data frame(as per the HW guidelines). <br>\n",
    "- After this, I separated my data into three classes (1, 2, 3) using a dictionary and added randomly shuffled the data to get different values at each execution. <br>\n",
    "- Finally, I picked 20000 rows from each of the three classes to create 60000 data set for further analysis.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94effe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d462a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data.tsv\",sep=\"\\t\",usecols = [\"review_body\",\"star_rating\"],on_bad_lines='skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4ab242",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=dataframe.dropna()\n",
    "\n",
    "ratingDict = {1:1, 2:1, 3:2, 4:3, 5:3}\n",
    "filteredDataFrame = dataframe.replace({\"star_rating\": ratingDict})\n",
    "filteredDataFrame = filteredDataFrame.sample(frac=1, random_state=14).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781964e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating1Data = filteredDataFrame[filteredDataFrame['star_rating']==1].head(20000)\n",
    "rating2Data = filteredDataFrame[filteredDataFrame['star_rating']==2].head(20000)\n",
    "rating3Data = filteredDataFrame[filteredDataFrame['star_rating']==3].head(20000)\n",
    "\n",
    "finalRatingsData = rating1Data.append(rating2Data).append(rating3Data).reset_index()\n",
    "finalRatingsData['star_rating']=finalRatingsData['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1d1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRatingsData['review_body'] = finalRatingsData[\"review_body\"].apply(str) #converts review_body column to string type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481b9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reviews length before data cleaning :  292.6594333333333\n"
     ]
    }
   ],
   "source": [
    "averageStringLenBeforeDataCleaning, stringLength = 0, 0\n",
    "for ratings in finalRatingsData['review_body']:\n",
    "  stringLength += len(ratings)\n",
    "\n",
    "averageStringLenBeforeDataCleaning = stringLength / len(finalRatingsData['review_body'])\n",
    "print(\"Average reviews length before data cleaning : \", averageStringLenBeforeDataCleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5214cf",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "For data cleaning, I have created a common function that will clean data for the following cases:<br>\n",
    "- Convert the reviews data into lowercase characters<br>\n",
    "- b. Remove numerical characters from the reviews data<br>\n",
    "- c. Remove punctuation marks from the reviews data<br>\n",
    "- d. Remove extra spaces from the reviews data<br>\n",
    "- e. Remove URLs from the reviews data<br>\n",
    "- f. Remove HTML tags from the reviews data<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1acda5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>this shampoo and conditioner left my hair feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>made my scalp itch so i sent it back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>i was shocked im still looking for my return l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>if this is real it is not as good as what i bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>not very polarized   tight fit gives me a head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>31753</td>\n",
       "      <td>3</td>\n",
       "      <td>best leave in conditioner i have ever used  i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>31754</td>\n",
       "      <td>3</td>\n",
       "      <td>i was opening the borage capsules and now i ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>31755</td>\n",
       "      <td>3</td>\n",
       "      <td>this is a wonderful fragrance  it is very allu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>31756</td>\n",
       "      <td>3</td>\n",
       "      <td>the most aromatic scent i have found thus far ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>31758</td>\n",
       "      <td>3</td>\n",
       "      <td>razor works great i have had it for over a yea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  star_rating                                        review_body\n",
       "0          6            1  this shampoo and conditioner left my hair feel...\n",
       "1          8            1               made my scalp itch so i sent it back\n",
       "2         20            1  i was shocked im still looking for my return l...\n",
       "3         24            1  if this is real it is not as good as what i bo...\n",
       "4         26            1  not very polarized   tight fit gives me a head...\n",
       "...      ...          ...                                                ...\n",
       "59995  31753            3  best leave in conditioner i have ever used  i ...\n",
       "59996  31754            3  i was opening the borage capsules and now i ju...\n",
       "59997  31755            3  this is a wonderful fragrance  it is very allu...\n",
       "59998  31756            3  the most aromatic scent i have found thus far ...\n",
       "59999  31758            3  razor works great i have had it for over a yea...\n",
       "\n",
       "[60000 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanReviewData(column):\n",
    "  column = column.lower() #converts string to lowercase\n",
    "  column = re.sub(r'\\d+','',column) #remove numerical characters from the string\n",
    "  column = re.sub(r'[^\\w\\s]','', column) #removes punctuations from the string\n",
    "  column = column.strip() #remove extra spaces from the string\n",
    "  column = column.replace('\\\\S*\\\\.com\\\\b','') #remove .com ending URLs from the string\n",
    "  column = column.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '') #remove http URLs from the string \n",
    "  column = column.replace('https?://\\S+|www\\.\\S+','') #remove www URLs from the string\n",
    "  column = column.replace('[\\w\\.-]+@[\\w\\.-]+\\.\\w', '') #remove email address from the string\n",
    "  column = column.replace('[^a-zA-Z ]', '') #remove non alphabetical characters from the string\n",
    "  column = column.replace('\\s+', ' ') \n",
    "    \n",
    "\n",
    "  return column\n",
    "\n",
    "def removehtml(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "finalRatingsData['review_body'] = finalRatingsData[\"review_body\"].apply(cleanReviewData)\n",
    "finalRatingsData['review_body'] = finalRatingsData[\"review_body\"].apply(lambda x: removehtml(x))\n",
    "finalRatingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59cfd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reviews length after data cleaning :  281.15886666666665\n"
     ]
    }
   ],
   "source": [
    "averageStringLengAfterDataCleaning, stringLength = 0, 0\n",
    "for ratings in finalRatingsData['review_body']:\n",
    "  stringLength += len(ratings)\n",
    "\n",
    "averageStringLengAfterDataCleaning = stringLength / len(finalRatingsData['review_body'])\n",
    "print(\"Average reviews length after data cleaning : \", averageStringLengAfterDataCleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c2ee2",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- For data preprocessing, I have used the NLTK package to first remove all the stop words from the dataset.\n",
    "- After this, I performed lemmatization using WordNetLemmatizer from the NLTK package.\n",
    "- And finally divided the dataset into training set (80%) and testing set (20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d61d16d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shwetakumari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "finalRatingsData['review_body'] = finalRatingsData['review_body'].apply(lambda key: ' '.join([word for word in key.split() if word not in (stop_words)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77a9df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shwetakumari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/shwetakumari/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "finalRatingsData['review_body'] = finalRatingsData['review_body'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791a505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reviews length after data preprocessing :  175.12863333333334\n"
     ]
    }
   ],
   "source": [
    "averageStringLengAfterDataPreprocessing, stringLength = 0, 0\n",
    "for ratings in finalRatingsData['review_body'].to_list():\n",
    "  stringLength += len(ratings)\n",
    "\n",
    "averageStringLengAfterDataPreprocessing = stringLength / len(finalRatingsData['review_body'])\n",
    "print(\"Average reviews length after data preprocessing : \", averageStringLengAfterDataPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "630a65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = finalRatingsData['review_body']\n",
    "Y = finalRatingsData['star_rating']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "Y_train = Y_train.reset_index(drop = True)\n",
    "Y_test = Y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926e6aa",
   "metadata": {},
   "source": [
    "# 2. Word Embeddings\n",
    "- First I loaded pre-trained Google News dataset Word2vec model and tested it on two sample examples.\n",
    "- Then I tested the model on three of my own examples to get the similarity score.\n",
    "- After that I trained Word2vec model on reviews dataset to get similarith score on the given dataset.\n",
    "- Results of both the cases are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df9b7",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f05f05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as genmodels\n",
    "import gensim.downloader as api\n",
    "googleW2V = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1eff8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n",
      "0.5567486\n"
     ]
    }
   ],
   "source": [
    "# Sample Examples\n",
    "\n",
    "print(googleW2V.most_similar(positive=['king','woman'], negative=['man'], topn=1))\n",
    "print(googleW2V.similarity('excellent', 'outstanding'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b6e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cats', 0.7502572536468506)]\n",
      "0.6632171\n",
      "0.5059545\n"
     ]
    }
   ],
   "source": [
    "# Three different examples\n",
    "\n",
    "print(googleW2V.most_similar(positive=['cat','kittens'], negative=['hamster'], topn=1))\n",
    "print(googleW2V.similarity('happy', 'pleased'))\n",
    "print(googleW2V.similarity('coffee', 'brew'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222329e",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed390ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_corpus = X_train\n",
    "for i in range(0, len(sent_corpus)):\n",
    "  sent_corpus[i] = sent_corpus[i].split(\" \")\n",
    "\n",
    "sent_corpus_test = X_test\n",
    "for i in range(0, len(sent_corpus_test)):\n",
    "  sent_corpus_test[i] = sent_corpus_test[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4957e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genmodels.Word2Vec(sentences=sent_corpus, vector_size=300, window=13, min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e04c327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('skinceuticals', 0.7845896482467651)]\n",
      "0.6814426\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['king','woman'], negative=['man'], topn=1))\n",
    "print(model.wv.similarity('excellent', 'outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33713425",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- Since we have used limited size of reviews dataset, the vocab size of Amazon's dataset will be much smaller than the pre-trained Google Word2vec model since it is trained on much larger dataset.\n",
    "- In the above example we can see the result given for king, woman example is more acurate by pre-trained Google Word2vec model i.e 'Queen' than my model i.e 'skinceuticals' since words like king, queen are not related to amazon's review dataset.\n",
    "- For the second example i.e 'execellet', 'outstanding' result given by my model is better than pre-trained model because possibility of repeating of these words in higher in amazon's dataset.\n",
    "- Therefore we can conclude that <b>pre-trained Word2Vec models</b> seems to encode semantic similarities between words better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b552f",
   "metadata": {},
   "source": [
    "# 3. Simple Models\n",
    "\n",
    "- Using the Google Word2vec features, I am performing the vector averaging on my X_train and X_test data for each review.\n",
    "- Firstly I am reading each review from the X_train and then checking each word of the review whether is it present in pre-trained model or not. If it is present then I'm appending it to a list. Once a review is parsed I'm taking mean of it if the length of vectors is greater than 0 else I'm appending zeroes to the list. Finally I'm appending the list to my result list.\n",
    "- I have repeated the same for X_test dataset.\n",
    "- Thereafter, I have trained and tested averaged word2vec features on Perceptron and SVM and reported the testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7971e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_model = []\n",
    "for sentence in sent_corpus:\n",
    "    singleReview = []\n",
    "    for word in sentence:\n",
    "        if word in googleW2V:\n",
    "            singleReview.append(googleW2V[word])\n",
    "    if len(singleReview) > 0:\n",
    "        sentence_avg = np.mean(singleReview, axis=0)\n",
    "    else:\n",
    "        sentence_avg = np.zeros((300,))\n",
    "    X_simple_model.append(sentence_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc7ca459",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_model_test = []\n",
    "for sentence in sent_corpus_test:\n",
    "    singleReview = []\n",
    "    for word in sentence:\n",
    "        if word in googleW2V:\n",
    "            singleReview.append(googleW2V[word])\n",
    "    if len(singleReview) > 0:\n",
    "        sentence_avg = np.mean(singleReview, axis=0)\n",
    "    else:\n",
    "        sentence_avg = np.zeros((300,))\n",
    "    X_simple_model_test.append(sentence_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd9e7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy obtained in SVM Model using Word Embeddings is: 62.8333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svmModel = LinearSVC()\n",
    "svmModel.fit(X_simple_model,Y_train)\n",
    "svmModelPredictions=svmModel.predict(X_simple_model_test)\n",
    "acc_svm=accuracy_score(Y_test,svmModelPredictions)\n",
    "print(\"The accuracy obtained in SVM Model using Word Embeddings is:\",round(acc_svm*100, 4),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70dbe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy obtained in SVM Model using TF-IDF is: 66.6872 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy obtained in SVM Model using TF-IDF is: 66.6872 %\") # Values taken from assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fce635e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy obtained in Perceptron Model using Word Embeddings is: 54.3333 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "perceptronModel = Perceptron()\n",
    "perceptronModel.fit(X_simple_model,Y_train)\n",
    "perceptronPredictions=perceptronModel.predict(X_simple_model_test)\n",
    "acc_perceptron=accuracy_score(Y_test,perceptronPredictions)\n",
    "print(\"The accuracy obtained in Perceptron Model using Word Embeddings is:\",round(acc_perceptron*100, 4),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5a32cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy obtained in Perceptron Model TF-IDF is: 59.7402 %\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy obtained in Perceptron Model TF-IDF is: 59.7402 %\")  # Values taken from assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4041ff6",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- As we can see the accuracy obtained by TF-IDF is better than Word Embeddings for both SVM and perceptron. The reason for this could be following:\n",
    "- TF-IDF reflects the importance of a word in a document and takes into account both the frequency of a word in a document and the frequency of the word across all documents. Word embeddings, on the other hand, represent words as dense vectors in a high-dimensional space\n",
    "- SVM and perceptron are linear models that rely on features that are linearly separable. TF-IDF scores are often more sparse and linearly separable than word embeddings, making them more suitable for linear models.\n",
    "- Also, pre-trained word embeddings are typically learned on very large datasets and may not capture the nuances of the specific dataset and small dataset like amazon reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ebec3",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks\n",
    "\n",
    "- I am using PyTorch for my implementation therefore initially I am importing all the required libraries and packages of PyTorch.\n",
    "- I'm changing my classes index from 1 to 0 then as required by PyTorch.\n",
    "- After that I have created a custom dataset which will be used by all of the models.\n",
    "- Then I have set hyperparameters like learning rate, vector size and epochs to be used MLP model. Then I am using my custom dataset to convert numpy array into tensors.\n",
    "- Finally using DataLoader functionality I am dividing my train and test split tensors into their batch sizes for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2eb97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32aa0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index ={} \n",
    "for i in range(1,4):\n",
    "    class_index[i]=i-1 \n",
    "Y_train.replace(class_index, inplace=True) \n",
    "Y_test.replace(class_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa20e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data \n",
    "        self.Y_data = Y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.Y_data[index]\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fd16e",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "- The input size for Feedforward Neural Network for Multilayer Perceptron is 300. The size for Hidden Layer 1 is 100, Hidden Layer 2 is 10 and for Output Layer is 3 as we have 3 classes. I have used relu as the activation function. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "- Then in forward propogation, I applies relu function on input data.\n",
    "- Then, after input layer the data is passed to Hidden Layer 1 and then Hidden Layer 2 and finally to the output layer. In each of these we apply relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f6a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_fnn = { 'lr': 0.0007, 'batch_size' : 10, 'epochs':13, 'vector_size' : 300,'num_classes' : 3 ,'hidden_size1' : 100,'hidden_size2' : 10}\n",
    "\n",
    "mlp_train_torch = ClassifierDataset(torch.from_numpy(np.array(X_simple_model)).float(), torch.from_numpy(Y_train.to_numpy()).long())\n",
    "mlp_test_torch = ClassifierDataset(torch.from_numpy(np.array(X_simple_model_test)).float(), torch.from_numpy(Y_test.to_numpy()).long())\n",
    "\n",
    "mlp_train_torch_loader = DataLoader(dataset=mlp_train_torch, batch_size=hyperparameters_fnn['batch_size'])\n",
    "mlp_test_torch_loader = DataLoader(dataset=mlp_test_torch, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f12a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, v_size, hidden_size1, hidden_size2, output_dim):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(v_size, hidden_size1) \n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2) \n",
    "        self.fcout = nn.Linear(hidden_size2, output_dim)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x=self.fc1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.fc2(x) \n",
    "        x=self.relu(x)\n",
    "        preds = self.fcout(x) \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbbba0c",
   "metadata": {},
   "source": [
    "- The I have created MLP model as an object of the main model network class. I am using cross entropy loss and Adams optimizer here.  Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "- Then I am defining some functions to calculate training and testing accuracy.\n",
    "- Then I trained my model for 13 epochs in batch size of 10 and learning rate 0.007. I do forward propogation, calculate the loss and accuracy and then perform backward propogation and optimization. Training accuracy after each epoch is calculated. zero_grad is used to reset all accumulated gradients after adam optimizer.\n",
    "- After successful training of model, I am calculating testing split accuracy using accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b68aba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP = MLP(hyperparameters_fnn['vector_size'],hyperparameters_fnn['hidden_size1'],hyperparameters_fnn['hidden_size2'],hyperparameters_fnn['num_classes']) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(modelMLP.parameters(),lr=hyperparameters_fnn['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "556bab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_training_accuracy(y_pred, y_test): \n",
    "    correct_pred = (y_pred== y_test)\n",
    "    acc = correct_pred.sum() / len(correct_pred) \n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc\n",
    "\n",
    "def calculate_testing_accuracy(y_pred, y_test): \n",
    "    acclist=[]\n",
    "    for i, j in y_test:\n",
    "        output = y_pred(i)\n",
    "        _, predicted = torch.max(output.data, 1) \n",
    "        correct_pred = (predicted== j)\n",
    "        acc = correct_pred.sum() / len(correct_pred) \n",
    "        acc = torch.round(acc * 100) \n",
    "        acclist.append(acc.item())\n",
    "    test_correct = sum(acclist)/len(y_test) \n",
    "    return test_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "215d79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.8087 \tTraining Accuracy: 63.6167\n",
      "Epoch: 2 \tTraining Loss: 0.7921 \tTraining Accuracy: 64.3958\n",
      "Epoch: 3 \tTraining Loss: 0.7793 \tTraining Accuracy: 65.1229\n",
      "Epoch: 4 \tTraining Loss: 0.7677 \tTraining Accuracy: 65.5938\n",
      "Epoch: 5 \tTraining Loss: 0.7568 \tTraining Accuracy: 66.0938\n",
      "Epoch: 6 \tTraining Loss: 0.7470 \tTraining Accuracy: 66.7500\n",
      "Epoch: 7 \tTraining Loss: 0.7374 \tTraining Accuracy: 67.2458\n",
      "Epoch: 8 \tTraining Loss: 0.7281 \tTraining Accuracy: 67.7208\n",
      "Epoch: 9 \tTraining Loss: 0.7189 \tTraining Accuracy: 68.2125\n",
      "Epoch: 10 \tTraining Loss: 0.7101 \tTraining Accuracy: 68.8833\n",
      "Epoch: 11 \tTraining Loss: 0.7011 \tTraining Accuracy: 69.2396\n",
      "Epoch: 12 \tTraining Loss: 0.6926 \tTraining Accuracy: 69.7042\n",
      "Epoch: 13 \tTraining Loss: 0.6841 \tTraining Accuracy: 70.1750\n"
     ]
    }
   ],
   "source": [
    "epochs = hyperparameters_fnn['epochs'] \n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    acclist=[]\n",
    "    modelMLP.train()\n",
    "    for data, target in mlp_train_torch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = modelMLP(data)\n",
    "        loss = criterion(output, target)\n",
    "        _,predicted = torch.max(output.data, 1) \n",
    "        acc=calculate_training_accuracy(predicted,target) \n",
    "        acclist.append(acc.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(mlp_train_torch_loader.dataset) \n",
    "    train_correct = sum(acclist)/len(mlp_train_torch_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f}'.format(epoch+1, train_loss,train_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc374da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy obtained in FNN with average word2vec is: 63.075\n"
     ]
    }
   ],
   "source": [
    "avg_fnn_accuracy = calculate_testing_accuracy(modelMLP, mlp_test_torch_loader)\n",
    "print(\"The testing accuracy obtained in FNN with average word2vec is:\",avg_fnn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086ef61",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "- In part b of FNN to generate input features I have just considered first 10 vectors and concatenated them.\n",
    "- In this case input size would be 3000 (300 * 10 words) and overall shape of word embeddings for train split is (48000,3000) and for test split is (12000,3000).\n",
    "- The rest of the code remains same for training the model and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b4806a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fnn_concat_train = []\n",
    "for sentence in sent_corpus:\n",
    "    singleReview = []\n",
    "    for word in sentence:\n",
    "        if word in googleW2V:\n",
    "            singleReview.append(googleW2V[word])\n",
    "        if len(singleReview) == 10:\n",
    "            break\n",
    "    if len(singleReview) < 10:\n",
    "        for i in range(len(singleReview),10):\n",
    "            singleReview.append(np.zeros((300,)))\n",
    "    singleReview = np.concatenate(singleReview, axis=0)\n",
    "    X_fnn_concat_train.append(singleReview)\n",
    "X_fnn_concat_train = np.array(X_fnn_concat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "306b2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fnn_concat_test = []\n",
    "for sentence in sent_corpus_test:\n",
    "    singleReview = []\n",
    "    for word in sentence:\n",
    "        if word in googleW2V:\n",
    "            singleReview.append(googleW2V[word])\n",
    "        if len(singleReview) == 10:\n",
    "            break\n",
    "    if len(singleReview) < 10:\n",
    "        for i in range(len(singleReview),10):\n",
    "            singleReview.append(np.zeros((300,)))\n",
    "    singleReview = np.concatenate(singleReview, axis=0)\n",
    "    X_fnn_concat_test.append(singleReview)\n",
    "X_fnn_concat_test = np.array(X_fnn_concat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25367060",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_fnn.update([('vector_size',3000)])\n",
    "\n",
    "mlp_train_torch_concat = ClassifierDataset(torch.from_numpy(np.array(X_fnn_concat_train)).float(), torch.from_numpy(Y_train.to_numpy()).long())\n",
    "mlp_test_torch_concat = ClassifierDataset(torch.from_numpy(np.array(X_fnn_concat_test)).float(), torch.from_numpy(Y_test.to_numpy()).long())\n",
    "\n",
    "mlp_train_torch_loader_concat = DataLoader(dataset=mlp_train_torch_concat, batch_size=hyperparameters_fnn['batch_size'])\n",
    "mlp_test_torch_loader_concat = DataLoader(dataset=mlp_test_torch_concat, batch_size=1)\n",
    "\n",
    "modelMLP2 = MLP(hyperparameters_fnn['vector_size'],hyperparameters_fnn['hidden_size1'],hyperparameters_fnn['hidden_size2'],hyperparameters_fnn['num_classes'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(modelMLP2.parameters(),lr=hyperparameters_fnn['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7d83af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.8329 \tTraining Accuracy: 61.7021\n",
      "Epoch: 2 \tTraining Loss: 0.7229 \tTraining Accuracy: 68.3875\n",
      "Epoch: 3 \tTraining Loss: 0.5738 \tTraining Accuracy: 76.2562\n",
      "Epoch: 4 \tTraining Loss: 0.4223 \tTraining Accuracy: 83.4500\n",
      "Epoch: 5 \tTraining Loss: 0.3119 \tTraining Accuracy: 88.1250\n",
      "Epoch: 6 \tTraining Loss: 0.2502 \tTraining Accuracy: 90.4292\n",
      "Epoch: 7 \tTraining Loss: 0.2129 \tTraining Accuracy: 91.7938\n",
      "Epoch: 8 \tTraining Loss: 0.1852 \tTraining Accuracy: 92.9104\n",
      "Epoch: 9 \tTraining Loss: 0.1640 \tTraining Accuracy: 93.7667\n",
      "Epoch: 10 \tTraining Loss: 0.1536 \tTraining Accuracy: 94.1833\n",
      "Epoch: 11 \tTraining Loss: 0.1381 \tTraining Accuracy: 94.8396\n",
      "Epoch: 12 \tTraining Loss: 0.1314 \tTraining Accuracy: 95.1896\n",
      "Epoch: 13 \tTraining Loss: 0.1193 \tTraining Accuracy: 95.5563\n"
     ]
    }
   ],
   "source": [
    "epochs = hyperparameters_fnn['epochs'] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_concat = 0.0 \n",
    "    accuracylist_concat=[]\n",
    "    modelMLP2.train()\n",
    "    for data, target in mlp_train_torch_loader_concat:\n",
    "        optimizer.zero_grad()\n",
    "        output = modelMLP2(data)\n",
    "        loss = criterion(output, target)\n",
    "        _,predicted = torch.max(output.data, 1) \n",
    "        acc=calculate_training_accuracy(predicted,target) \n",
    "        accuracylist_concat.append(acc.item()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_concat += loss.item()*data.size(0)\n",
    "    train_loss_concat = train_loss_concat/len(mlp_train_torch_loader_concat.dataset) \n",
    "    train_correct_concat = sum(accuracylist_concat)/len(mlp_train_torch_loader_concat)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f}'.format(epoch+1, train_loss_concat, train_correct_concat))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be936ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy obtained in FNN with concatenated word2vec of length 10 is: 51.65833333333333\n"
     ]
    }
   ],
   "source": [
    "fnn_accuracy_concat = calculate_testing_accuracy(modelMLP2, mlp_test_torch_loader_concat)\n",
    "print(\"The testing accuracy obtained in FNN with concatenated word2vec of length 10 is:\",fnn_accuracy_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd0d3b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- The accuracies obtained in both the cases 4(a) using weighted average and 4(b) using first 10 words only is better than Single Layer Perceptron that we are using in Simple Models. The reason being that in FNN Multilayer Perceptron we have 2 hidden layers which helps in better optimization of model through forward and backward propogation.\n",
    "- After comparing the accuracies of 4(a) and 4(b) with those Single Layer Perceptron used in Simple Models, we can see that it is better for SVM and similar for Perceptron because in FNN Multilayer Perceptron we have 2 hidden layers which helps in better optimization of model through forward and backward propogation.\n",
    "- Additionally, comparing 4(a) and 4(b) testing accuracy in part 4(a) is better than part (b). This is because we are using average of all the words in the review which captures the essence of all the important words in that review whereas in part (b) there is a possibility of not considering important words in the review as they may not be the first 10 words which results in loss of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9fff9",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks\n",
    "\n",
    "- For RNN to I have created the input features by appending of first 20 words of each review. If the length of review is shorter than 20 I am padding it zero values.\n",
    "- The overal size of train split using this word embedding is (48000,20,300) and test split is (12000,20,300). \n",
    "- Also, I have used the same similar hyperparameter as that of FNN for these models to get an accurate comparison between the two.\n",
    "- The rest of the code is similar to FNN except the model name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "497a63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn_concat_train = []\n",
    "for sentence in sent_corpus:\n",
    "    singleReview = []\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in googleW2V:\n",
    "            singleReview.append(googleW2V[sentence[i]])\n",
    "        if len(singleReview) == 20:\n",
    "            break\n",
    "    if len(singleReview) < 20:\n",
    "        for i in range(len(singleReview),20):\n",
    "            singleReview.append(np.zeros((300,)))\n",
    "    singleReview = np.array(singleReview)\n",
    "    X_rnn_concat_train.append(singleReview)\n",
    "X_rnn_concat_train = np.array(X_rnn_concat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ca007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn_concat_test = []\n",
    "for sentence in sent_corpus_test:\n",
    "    singleReview = []\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in googleW2V:\n",
    "            singleReview.append(googleW2V[sentence[i]])\n",
    "        if len(singleReview) == 20:\n",
    "            break\n",
    "    if len(singleReview) < 20:\n",
    "        for i in range(len(singleReview),20):\n",
    "            singleReview.append(np.zeros((300,)))\n",
    "    singleReview = np.array(singleReview)\n",
    "    X_rnn_concat_test.append(singleReview)\n",
    "X_rnn_concat_test = np.array(X_rnn_concat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "332acc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_rnn={ 'lr': 0.0007, 'batch_size' : 10, 'epochs':9, 'vector_size' : 300, 'dropout': 0.2,'num_classes' : 3 ,'hidden_size' : 20}\n",
    "\n",
    "rnn_train_torch = ClassifierDataset(torch.from_numpy(np.array(X_rnn_concat_train)).float(), torch.from_numpy(Y_train.to_numpy()).long()) \n",
    "rnn_test_torch = ClassifierDataset(torch.from_numpy(np.array(X_rnn_concat_test)).float(), torch.from_numpy(Y_test.to_numpy()).long())\n",
    "\n",
    "rnn_train_torch_loader= DataLoader(dataset=rnn_train_torch, batch_size=hyperparameters_rnn['batch_size'])\n",
    "rnn_test_torch_loader= DataLoader(dataset=rnn_test_torch, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739edd0d",
   "metadata": {},
   "source": [
    "### a) RNN\n",
    "\n",
    "- In this RNN model we have an RNN cell with the hidden state size of 20. The input size is 300 and the output size is 3. \n",
    "- I haved used nn.RNN model available to us through PyTorch. Then I create the object of our RNN model and use the same cross entropy loss and optimizer.\n",
    "- Then I trained my model for 9 epochs in batch size of 10 and learning rate 0.007. I have used similar training method as in FNN.\n",
    "- After successful training of model, I am calculating testing split accuracy using accuracy function. Finally the accuracy for testing split is also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "95463253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRNN(nn.Module):\n",
    "    def __init__(self, v_size=300, hidden_size=20, num_classes=3):\n",
    "        super(ModelRNN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.input_dim= v_size\n",
    "        self.output_dim = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.RNN(v_size, hidden_size, batch_first=True) \n",
    "        self.fc2 = nn.Linear(hidden_size ,num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = torch.zeros(1,batch_size, self.hidden_size) \n",
    "        op,h1=self.fc1(x,hidden)\n",
    "        output = self.fc2(op[:,-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26cb998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelRNN(hyperparameters_rnn['vector_size'],hyperparameters_rnn['hidden_size'],hyperparameters_fnn['num_classes']) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=hyperparameters_rnn['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53176d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.1193 \tTraining Accuracy: 49.3417\n",
      "Epoch: 2 \tTraining Loss: 0.1193 \tTraining Accuracy: 57.1146\n",
      "Epoch: 3 \tTraining Loss: 0.1193 \tTraining Accuracy: 58.2396\n",
      "Epoch: 4 \tTraining Loss: 0.1193 \tTraining Accuracy: 59.0271\n",
      "Epoch: 5 \tTraining Loss: 0.1193 \tTraining Accuracy: 59.6042\n",
      "Epoch: 6 \tTraining Loss: 0.1193 \tTraining Accuracy: 60.6271\n",
      "Epoch: 7 \tTraining Loss: 0.1193 \tTraining Accuracy: 61.2917\n",
      "Epoch: 8 \tTraining Loss: 0.1193 \tTraining Accuracy: 61.3188\n",
      "Epoch: 9 \tTraining Loss: 0.1193 \tTraining Accuracy: 61.6854\n"
     ]
    }
   ],
   "source": [
    "epochs = hyperparameters_rnn['epochs'] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_rnn = 0.0\n",
    "    accuracylist_rnn=[]\n",
    "    model.train()\n",
    "    for data, target in rnn_train_torch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor(np.array(data),dtype=torch.float) \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _,predicted = torch.max(output.data, 1) \n",
    "        acc=calculate_training_accuracy(predicted,target) \n",
    "        accuracylist_rnn.append(acc.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_rnn += loss.item()*data.size(0)\n",
    "    train_loss_rnn = train_loss_rnn/len(rnn_train_torch_loader.dataset) \n",
    "    train_correct_rnn = sum(accuracylist_rnn)/len(rnn_train_torch_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f}'.format(epoch+1, train_loss_concat,train_correct_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f69cda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy obtained in RNN with word2vec of length 20 is:  57.275\n"
     ]
    }
   ],
   "source": [
    "rnn_accuracy = calculate_testing_accuracy(model, rnn_test_torch_loader)\n",
    "print(\"The testing accuracy obtained in RNN with word2vec of length 20 is: \", rnn_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6c71e",
   "metadata": {},
   "source": [
    "##### RNN Conclusion\n",
    "\n",
    "- The accuracy obtained using FNN with weighted average is better than RNN with using first 20 words since in FNN we are considering weighted average of all the words in the review than first 20 words of the review. Also, in RNN the previous word can affect the value of next word as the output of first word goes to next word.\n",
    "- The accuracy obtained using RNN using first 20 words is better than FNN with first 10 words since for longer reviews i.e 20 instead of 10, it may capture the word embeddings better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379bec",
   "metadata": {},
   "source": [
    "### b) GRU\n",
    "\n",
    "- The code for GRU is similar to RNN with the only differenc being in using nn.GRU instead of nn.RNN.\n",
    "- Thus, I have defined the model in GRU with same forward propogation, hyperparamteres and performing training as well with the same method. At the end, I am calculating the testing accuracy for testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "62f2e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGRU(nn.Module):\n",
    "    def __init__(self, v_size=300, hidden_size=20, num_classes=3):\n",
    "        super(ModelGRU, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.input_dim= v_size\n",
    "        self.output_dim = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = nn.GRU(v_size, hidden_size, batch_first=True) \n",
    "        self.fc2 = nn.Linear(hidden_size ,num_classes)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = torch.zeros(1,batch_size, self.hidden_size) \n",
    "        op,h1=self.fc1(x,hidden)\n",
    "        output = self.fc2(op[:,-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22b74df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelGRU(hyperparameters_rnn['vector_size'],hyperparameters_rnn['hidden_size'],hyperparameters_rnn['num_classes']) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=hyperparameters_rnn['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "452b1b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.8965 \tTraining Accuracy: 55.6708\n",
      "Epoch: 2 \tTraining Loss: 0.7966 \tTraining Accuracy: 63.9417\n",
      "Epoch: 3 \tTraining Loss: 0.7676 \tTraining Accuracy: 65.6229\n",
      "Epoch: 4 \tTraining Loss: 0.7490 \tTraining Accuracy: 66.5187\n",
      "Epoch: 5 \tTraining Loss: 0.7348 \tTraining Accuracy: 67.2333\n",
      "Epoch: 6 \tTraining Loss: 0.7228 \tTraining Accuracy: 67.9604\n",
      "Epoch: 7 \tTraining Loss: 0.7121 \tTraining Accuracy: 68.4562\n",
      "Epoch: 8 \tTraining Loss: 0.7020 \tTraining Accuracy: 68.9667\n",
      "Epoch: 9 \tTraining Loss: 0.6925 \tTraining Accuracy: 69.5104\n"
     ]
    }
   ],
   "source": [
    "epochs = hyperparameters_rnn['epochs'] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_gru = 0.0\n",
    "    accuracylist_gru=[]\n",
    "    model.train()\n",
    "    for data, target in rnn_train_torch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor(np.array(data),dtype=torch.float) \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _,predicted = torch.max(output.data, 1) \n",
    "        acc=calculate_training_accuracy(predicted,target) \n",
    "        accuracylist_gru.append(acc.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_gru += loss.item()*data.size(0)\n",
    "    train_loss_gru = train_loss_gru/len(rnn_train_torch_loader.dataset) \n",
    "    train_correct_gru = sum(accuracylist_gru)/len(rnn_train_torch_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f}'.format(epoch+1, train_loss_gru,train_correct_gru ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77340c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy obtained in GRU with word2vec of length 20 is:  64.15\n"
     ]
    }
   ],
   "source": [
    "gru_accuracy = calculate_testing_accuracy(model, rnn_test_torch_loader)\n",
    "print(\"The testing accuracy obtained in GRU with word2vec of length 20 is: \", gru_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf12434",
   "metadata": {},
   "source": [
    "### c) LSTM\n",
    "\n",
    "- The code for LSTM is similar to RNN and GRU with the only difference being in using nn.GRU instead of nn.RNN and I have used same hyperparameters for all three models..\n",
    "- At the end, I am calculating the testing accuracy for testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a8fa55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ModelLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden_state = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        cell_state = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        lstm_output, (hidden_state, cell_state) = self.lstm(x, (hidden_state, cell_state))\n",
    "        output = self.fc(lstm_output[:, -1, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bebd18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLSTM(hyperparameters_rnn['vector_size'],hyperparameters_rnn['hidden_size'],hyperparameters_rnn['num_classes']) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=hyperparameters_rnn['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d44d157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.9114 \tTraining Accuracy: 54.5708\n",
      "Epoch: 2 \tTraining Loss: 0.8240 \tTraining Accuracy: 62.1667\n",
      "Epoch: 3 \tTraining Loss: 0.7856 \tTraining Accuracy: 64.4688\n",
      "Epoch: 4 \tTraining Loss: 0.7611 \tTraining Accuracy: 65.8083\n",
      "Epoch: 5 \tTraining Loss: 0.7425 \tTraining Accuracy: 66.8104\n",
      "Epoch: 6 \tTraining Loss: 0.7266 \tTraining Accuracy: 67.8000\n",
      "Epoch: 7 \tTraining Loss: 0.7122 \tTraining Accuracy: 68.6833\n",
      "Epoch: 8 \tTraining Loss: 0.6989 \tTraining Accuracy: 69.5417\n",
      "Epoch: 9 \tTraining Loss: 0.6862 \tTraining Accuracy: 70.1896\n"
     ]
    }
   ],
   "source": [
    "epochs = hyperparameters_rnn['epochs'] \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss_lstm = 0.0\n",
    "    accuracylist_lstm=[]\n",
    "    model.train()\n",
    "    for data, target in rnn_train_torch_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor(np.array(data),dtype=torch.float) \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        _,predicted = torch.max(output.data, 1) \n",
    "        acc=calculate_training_accuracy(predicted,target) \n",
    "        accuracylist_lstm.append(acc.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_lstm += loss.item()*data.size(0)\n",
    "    train_loss_lstm = train_loss_lstm/len(rnn_train_torch_loader.dataset) \n",
    "    train_correct_lstm = sum(accuracylist_lstm)/len(rnn_train_torch_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f}'.format(epoch+1, train_loss_lstm,train_correct_lstm ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58fa30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy obtained in LSTM with word2vec of length 20 is:  63.916666666666664\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy = calculate_testing_accuracy(model, rnn_test_torch_loader)\n",
    "print(\"The testing accuracy obtained in LSTM with word2vec of length 20 is: \", lstm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdffea9",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "- From the above values we can see that GRU performs better than RNN and LSTM and the testing accuracy obtained in GRU is more than RNN as well as LSTM because of the following reasons:\n",
    "- GRU overcomes the problem of gradient vanishing problem whereas RNNs and LSTMs are prone to the problem of vanishing and exploding gradients.\n",
    "- LSTMs can be prone to overfitting because they have a large number of parameters that can be tuned. GRUs, on the other hand, have fewer parameters and are therefore less likely to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b05b8",
   "metadata": {},
   "source": [
    "# Final Values Of All Models\n",
    "\n",
    "#### Simple Models\n",
    "- Accuracy for SVM: 62.8333 %\n",
    "- Accuracy for single layer Perceptron: 54.3333 %\n",
    "\n",
    "#### Feedforward Neural Networks\n",
    "- Accuracy for  MLP with average word embeddings: 63.075 %\n",
    "- Accuracy for  MLP with concatenated word embeddings: 51.6583 %\n",
    "\n",
    "#### Recurrent Neural Networks\n",
    "- Accuracy for RNN: 57.275 %\n",
    "- Accuracy for GRU: 64.15 %\n",
    "- Accuracy for LSTM: 63.91666 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b315fb7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "- https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "- https://dipikabaad.medium.com/finding-the-hidden-sentiments-using-rnns-in-pytorch-f1e1e9638e9c\n",
    "- https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab\n",
    "- https://subscription.packtpub.com/book/data/9781789614381/6/ch06lvl1sec28/training-rnns-for-sentiment-analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
